{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Fingerprinting and Matching Demo\n",
    "\n",
    "This Jupyter notebook demonstrates **audio fingerprinting** and **audio matching** for the **RadioPlay** royalty payment system, designed to monitor radio plays for Ghanaian artists. The system identifies songs from 10-second audio clips, similar to Shazam, by generating unique fingerprints and matching them against a database. This demo allows you to:\n",
    "\n",
    "- Generate fingerprints from audio files (full songs and clips).\n",
    "- Visualize spectrograms, waveforms, and detected peaks.\n",
    "- Automatically find the best parameter configuration for peak detection and matching.\n",
    "- Match clips against a song database.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Audio Files**: Place one full song (WAV) and a 10-second clip (WAV) in a ./audio folder. Example: `song1.wav`, `clip1.wav`.\n",
    "- **Dependencies**: Install required Python packages.\n",
    "- **Environment**: Run in a Jupyter notebook with access to ffmpeg for audio processing.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install dependencies and create an audio folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e25003",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa numba xxhash matplotlib numpy scipy pandas\n",
    "!mkdir -p ./audio\n",
    "# Convert MP3 to WAV if needed: ffmpeg -i ./audio/song1.mp3 ./audio/song1.wav\n",
    "# Example clip extraction: ffmpeg -i ./audio/song1.wav -ss 60 -t 10 -c copy ./audio/clip1.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0773d",
   "metadata": {},
   "source": [
    "## Step 1: Define Fingerprinting Engine\n",
    "\n",
    "We use a fingerprinting engine based on librosa for spectrograms, numba for fast peak detection, and xxhash for hashing.\n",
    "\n",
    "**Modifications**:\n",
    "- Enhanced logging for peaks and hashes.\n",
    "- Configurable parameters for grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac715bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit\n",
    "import xxhash\n",
    "from operator import itemgetter\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'DEFAULT_FS': 44100,\n",
    "    'DEFAULT_WINDOW_SIZE': 2048,\n",
    "    'DEFAULT_OVERLAP_RATIO': 0.5,\n",
    "    'DEFAULT_FAN_VALUE': 15,\n",
    "    'DEFAULT_AMP_MIN': -10,\n",
    "    'PEAK_NEIGHBORHOOD_SIZE': 10,\n",
    "    'MIN_HASH_TIME_DELTA': 0,\n",
    "    'MAX_HASH_TIME_DELTA': 500,\n",
    "    'FINGERPRINT_REDUCTION': 20,\n",
    "    'PEAK_SORT': True\n",
    "}\n",
    "\n",
    "def fingerprint(channel_samples: np.ndarray, Fs: int = CONFIG['DEFAULT_FS'], \n",
    "                wsize: int = CONFIG['DEFAULT_WINDOW_SIZE'], wratio: float = CONFIG['DEFAULT_OVERLAP_RATIO'],\n",
    "                fan_value: int = CONFIG['DEFAULT_FAN_VALUE'], amp_min: float = CONFIG['DEFAULT_AMP_MIN'],\n",
    "                peak_neighborhood_size: int = CONFIG['PEAK_NEIGHBORHOOD_SIZE']) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Generate fingerprints from audio samples.\"\"\"\n",
    "    try:\n",
    "        samples = channel_samples.astype(np.float32) / 32768.0\n",
    "        hop_length = int(wsize * (1 - wratio))\n",
    "        S = librosa.stft(samples, n_fft=wsize, hop_length=hop_length, window='hann')\n",
    "        arr2D = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "        logger.info(f\"Spectrogram min: {arr2D.min():.2f}, max: {arr2D.max():.2f}\")\n",
    "        local_maxima = get_2D_peaks(arr2D, amp_min=amp_min, peak_neighborhood_size=peak_neighborhood_size)\n",
    "        hashes = generate_hashes(local_maxima, fan_value=fan_value)\n",
    "        logger.info(f\"Generated {len(hashes)} fingerprints for {len(samples)/Fs:.2f}s audio\")\n",
    "        return hashes\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fingerprinting failed: {e}\")\n",
    "        return []\n",
    "\n",
    "@jit(nopython=True)\n",
    "def get_2D_peaks_numba(arr2D: np.ndarray, amp_min: float, peak_neighborhood_size: int) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Optimized peak detection with numba.\"\"\"\n",
    "    peaks = []\n",
    "    rows, cols = arr2D.shape\n",
    "    neighborhood_size = peak_neighborhood_size // 2\n",
    "    for i in range(neighborhood_size, rows - neighborhood_size):\n",
    "        for j in range(neighborhood_size, cols - neighborhood_size):\n",
    "            if arr2D[i, j] > amp_min:\n",
    "                is_max = True\n",
    "                for di in range(-neighborhood_size, neighborhood_size + 1):\n",
    "                    for dj in range(-neighborhood_size, neighborhood_size + 1):\n",
    "                        if di == 0 and dj == 0:\n",
    "                            continue\n",
    "                        if arr2D[i + di, j + dj] > arr2D[i, j]:\n",
    "                            is_max = False\n",
    "                            break\n",
    "                    if not is_max:\n",
    "                        break\n",
    "                if is_max:\n",
    "                    peaks.append((i, j))\n",
    "    return peaks\n",
    "\n",
    "def get_2D_peaks(arr2D: np.ndarray, plot: bool = False, amp_min: float = CONFIG['DEFAULT_AMP_MIN'], \n",
    "                 peak_neighborhood_size: int = CONFIG['PEAK_NEIGHBORHOOD_SIZE']) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Extract peaks from spectrogram.\"\"\"\n",
    "    try:\n",
    "        peaks = get_2D_peaks_numba(arr2D, amp_min, peak_neighborhood_size)\n",
    "        logger.info(f\"Detected {len(peaks)} peaks with amp_min={amp_min}\")\n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(arr2D, origin='lower', aspect='auto', cmap='viridis')\n",
    "            if peaks:\n",
    "                freqs, times = zip(*peaks)\n",
    "                plt.scatter(times, freqs, c='r', s=10, label='Peaks')\n",
    "            plt.colorbar(label='Amplitude (dB)')\n",
    "            plt.xlabel('Time (frames)')\n",
    "            plt.ylabel('Frequency (bins)')\n",
    "            plt.title(f'Spectrogram with Detected Peaks (amp_min={amp_min})')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        return peaks\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Peak detection failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_hashes(peaks: List[Tuple[int, int]], fan_value: int = CONFIG['DEFAULT_FAN_VALUE']) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Generate hashes from peaks.\"\"\"\n",
    "    try:\n",
    "        if CONFIG['PEAK_SORT']:\n",
    "            peaks.sort(key=itemgetter(1))\n",
    "        hashes = []\n",
    "        valid_pairs = 0\n",
    "        for i in range(len(peaks)):\n",
    "            for j in range(1, fan_value):\n",
    "                if (i + j) < len(peaks):\n",
    "                    freq1 = peaks[i][0]\n",
    "                    freq2 = peaks[i + j][0]\n",
    "                    t1 = peaks[i][1]\n",
    "                    t2 = peaks[i + j][1]\n",
    "                    t_delta = t2 - t1\n",
    "                    if CONFIG['MIN_HASH_TIME_DELTA'] <= t_delta <= CONFIG['MAX_HASH_TIME_DELTA']:\n",
    "                        valid_pairs += 1\n",
    "                        h = xxhash.xxh64(f\"{freq1}|{freq2}|{t_delta}\".encode('utf-8'))\n",
    "                        hash_str = h.hexdigest()[:CONFIG['FINGERPRINT_REDUCTION']]\n",
    "                        hashes.append((hash_str, t1))\n",
    "        logger.info(f\"Generated {valid_pairs} valid peak pairs for hashing\")\n",
    "        return hashes\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Hash generation failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aec36b",
   "metadata": {},
   "source": [
    "## Step 2: Load Audio Files\n",
    "\n",
    "Load a full song and a 10-second clip to simulate radio monitoring.\n",
    "\n",
    "**Modifications**:\n",
    "- Use WAV format for song (`song1.wav`).\n",
    "- Enhanced logging and waveform visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3330a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_audio(file_path: str) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Load audio file and convert to int16.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return np.array([]), 0\n",
    "        samples, sr = librosa.load(file_path, sr=CONFIG['DEFAULT_FS'], mono=True)\n",
    "        samples = (samples * 32768).astype(np.int16)\n",
    "        logger.info(f\"Loaded {file_path}: {len(samples)} samples, {sr} Hz, max amplitude: {np.max(np.abs(samples))}\")\n",
    "        return samples, sr\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load {file_path}: {e}\")\n",
    "        return np.array([]), 0\n",
    "\n",
    "# Audio files\n",
    "song_path = './audio/song1.wav'  # Full song (WAV)\n",
    "clip_path = './audio/clip1.wav'  # 10-second clip\n",
    "\n",
    "song_samples, song_sr = load_audio(song_path)\n",
    "clip_samples, clip_sr = load_audio(clip_path)\n",
    "\n",
    "print(f\"Loaded song: {len(song_samples)/song_sr:.2f}s, {song_sr} Hz\")\n",
    "print(f\"Loaded clip: {len(clip_samples)/clip_sr:.2f}s, {clip_sr} Hz\")\n",
    "\n",
    "# Visualize clip waveform\n",
    "if len(clip_samples) > 0:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(clip_samples)\n",
    "    plt.title('Clip Waveform')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef102e",
   "metadata": {},
   "source": [
    "## Step 3: Define Song and Matching Functions\n",
    "\n",
    "Define the song model and matching logic.\n",
    "\n",
    "**Modifications**:\n",
    "- Separated song fingerprinting and matching for grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207faaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Django Song model\n",
    "class Song:\n",
    "    def __init__(self, id, title):\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "\n",
    "# Create a song entry\n",
    "song = Song(id=1, title='Sample Song')\n",
    "\n",
    "def generate_song_fingerprints(samples, sr, song_id, amp_min=CONFIG['DEFAULT_AMP_MIN'], \n",
    "                               peak_neighborhood_size=CONFIG['PEAK_NEIGHBORHOOD_SIZE'], \n",
    "                               fan_value=CONFIG['DEFAULT_FAN_VALUE'], plot=False):\n",
    "    \"\"\"Generate fingerprints for a song.\"\"\"\n",
    "    if len(samples) == 0:\n",
    "        logger.error(\"No samples provided for fingerprinting\")\n",
    "        return []\n",
    "    fingerprints = fingerprint(samples, Fs=sr, amp_min=amp_min, peak_neighborhood_size=peak_neighborhood_size, \n",
    "                              fan_value=fan_value)\n",
    "    db_fingerprints = [(song_id, h, o) for h, o in fingerprints]\n",
    "    if plot:\n",
    "        samples_float = samples.astype(np.float32) / 32768.0\n",
    "        hop_length = int(CONFIG['DEFAULT_WINDOW_SIZE'] * (1 - CONFIG['DEFAULT_OVERLAP_RATIO']))\n",
    "        S = librosa.stft(samples_float, n_fft=CONFIG['DEFAULT_WINDOW_SIZE'], hop_length=hop_length, window='hann')\n",
    "        arr2D = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "        get_2D_peaks(arr2D, plot=True, amp_min=amp_min, peak_neighborhood_size=peak_neighborhood_size)\n",
    "    return db_fingerprints\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def match_clip(clip_samples, clip_sr, db_fingerprints, amp_min=CONFIG['DEFAULT_AMP_MIN'], \n",
    "               fan_value=CONFIG['DEFAULT_FAN_VALUE'], peak_neighborhood_size=CONFIG['PEAK_NEIGHBORHOOD_SIZE']):\n",
    "    \"\"\"Match clip fingerprints against database.\"\"\"\n",
    "    if len(clip_samples) == 0:\n",
    "        return {\"match\": False, \"reason\": \"No samples in clip\"}\n",
    "    \n",
    "    # Visualize clip waveform\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(clip_samples)\n",
    "    plt.title('Clip Waveform')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute spectrogram for visualization\n",
    "    samples_float = clip_samples.astype(np.float32) / 32768.0\n",
    "    hop_length = int(CONFIG['DEFAULT_WINDOW_SIZE'] * (1 - CONFIG['DEFAULT_OVERLAP_RATIO']))\n",
    "    S = librosa.stft(samples_float, n_fft=CONFIG['DEFAULT_WINDOW_SIZE'], hop_length=hop_length, window='hann')\n",
    "    arr2D = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "    logger.info(f\"Clip spectrogram min: {arr2D.min():.2f}, max: {arr2D.max():.2f}\")\n",
    "    \n",
    "    # Generate clip fingerprints\n",
    "    clip_fingerprints = fingerprint(clip_samples, Fs=clip_sr, amp_min=amp_min, fan_value=fan_value, \n",
    "                                    peak_neighborhood_size=peak_neighborhood_size)\n",
    "    \n",
    "    # Visualize spectrogram and peaks\n",
    "    get_2D_peaks(arr2D, plot=True, amp_min=amp_min, peak_neighborhood_size=peak_neighborhood_size)\n",
    "    \n",
    "    if not clip_fingerprints:\n",
    "        return {\"match\": False, \"reason\": \"No fingerprints extracted\", \"hashes_matched\": 0, \n",
    "                \"input_confidence\": 0.0, \"db_confidence\": 0.0}\n",
    "    \n",
    "    # Match fingerprints\n",
    "    query_hashes = [h for h, _ in clip_fingerprints]\n",
    "    db_hashes = {(h, o, song_id) for song_id, h, o in db_fingerprints if h in query_hashes}\n",
    "    if not db_hashes:\n",
    "        return {\"match\": False, \"reason\": \"No matching hashes found\", \"hashes_matched\": 0, \n",
    "                \"input_confidence\": 0.0, \"db_confidence\": 0.0}\n",
    "    \n",
    "    match_map = Counter()\n",
    "    for h, query_offset in clip_fingerprints:\n",
    "        for db_hash, db_offset, song_id in db_hashes:\n",
    "            if h == db_hash:\n",
    "                offset_diff = db_offset - query_offset\n",
    "                match_map[(song_id, offset_diff)] += 1\n",
    "    \n",
    "    if not match_map:\n",
    "        return {\"match\": False, \"reason\": \"No offset alignment found\", \"hashes_matched\": 0, \n",
    "                \"input_confidence\": 0.0, \"db_confidence\": 0.0}\n",
    "    \n",
    "    (song_id, offset_diff), match_count = match_map.most_common(1)[0]\n",
    "    total_query_hashes = len(query_hashes)\n",
    "    total_db_hashes = sum(1 for _, _, sid in db_fingerprints if sid == song_id)\n",
    "    input_confidence = (match_count / total_query_hashes) * 100\n",
    "    db_confidence = (match_count / total_db_hashes) * 100 if total_db_hashes else 0\n",
    "    \n",
    "    # Thresholds\n",
    "    MIN_MATCH_COUNT = 10\n",
    "    MIN_INPUT_CONF = 10.0\n",
    "    MIN_DB_CONF = 2.0\n",
    "    \n",
    "    if match_count < MIN_MATCH_COUNT or input_confidence < MIN_INPUT_CONF or db_confidence < MIN_DB_CONF:\n",
    "        return {\n",
    "            \"match\": False,\n",
    "            \"reason\": \"Low confidence match\",\n",
    "            \"hashes_matched\": match_count,\n",
    "            \"input_confidence\": input_confidence,\n",
    "            \"db_confidence\": db_confidence\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"match\": True,\n",
    "        \"song_id\": song_id,\n",
    "        \"offset\": offset_diff,\n",
    "        \"hashes_matched\": match_count,\n",
    "        \"input_confidence\": input_confidence,\n",
    "        \"db_confidence\": db_confidence\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Grid Search for Optimal Parameters\n",
    "\n",
    "Perform a grid search over parameter combinations to find the best configuration for fingerprinting and matching.\n",
    "\n",
    "**Metrics**:\n",
    "- Number of peaks detected (clip and song).\n",
    "- Number of fingerprints generated (clip and song).\n",
    "- Matching confidence (input_confidence, db_confidence).\n",
    "- Successful match (match: True).\n",
    "\n",
    "**Scoring**:\n",
    "- Score = (clip_fingerprints / max_clip_fingerprints) * 0.3 + (song_fingerprints / max_song_fingerprints) * 0.3 + input_confidence * 0.2 + db_confidence * 0.2 if match is True, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# Define parameter ranges\n",
    "param_grid = {\n",
    "    'amp_min': [-40, -30, -20, -10, -5],  # Negative values for dB scale\n",
    "    'peak_neighborhood_size': [3, 5, 10, 15],\n",
    "    'fan_value': [10, 15, 20]\n",
    "}\n",
    "\n",
    "# Function to compute fingerprints and match\n",
    "def evaluate_params(song_samples, song_sr, clip_samples, clip_sr, song_id, params):\n",
    "    amp_min = params['amp_min']\n",
    "    peak_neighborhood_size = params['peak_neighborhood_size']\n",
    "    fan_value = params['fan_value']\n",
    "    \n",
    "    # Generate song fingerprints\n",
    "    song_fingerprints = generate_song_fingerprints(song_samples, song_sr, song_id, amp_min=amp_min, \n",
    "                                                  peak_neighborhood_size=peak_neighborhood_size, \n",
    "                                                  fan_value=fan_value, plot=False)\n",
    "    \n",
    "    # Generate clip fingerprints and match\n",
    "    result = match_clip(clip_samples, clip_sr, song_fingerprints, amp_min=amp_min, fan_value=fan_value, \n",
    "                        peak_neighborhood_size=peak_neighborhood_size)\n",
    "    \n",
    "    # Compute metrics\n",
    "    clip_peaks = 0\n",
    "    clip_fingerprints = len([h for h, _ in fingerprint(clip_samples, Fs=clip_sr, amp_min=amp_min, \n",
    "                                                       fan_value=fan_value, \n",
    "                                                       peak_neighborhood_size=peak_neighborhood_size)])\n",
    "    song_peaks = 0\n",
    "    song_fingerprints = len(song_fingerprints)\n",
    "    \n",
    "    return {\n",
    "        'amp_min': amp_min,\n",
    "        'peak_neighborhood_size': peak_neighborhood_size,\n",
    "        'fan_value': fan_value,\n",
    "        'clip_peaks': clip_peaks,\n",
    "        'clip_fingerprints': clip_fingerprints,\n",
    "        'song_peaks': song_peaks,\n",
    "        'song_fingerprints': song_fingerprints,\n",
    "        'match': result['match'],\n",
    "        'hashes_matched': result.get('hashes_matched', 0),\n",
    "        'input_confidence': result.get('input_confidence', 0.0),\n",
    "        'db_confidence': result.get('db_confidence', 0.0),\n",
    "        'reason': result.get('reason', '')\n",
    "    }\n",
    "\n",
    "# Run grid search\n",
    "results = []\n",
    "param_combinations = list(product(param_grid['amp_min'], param_grid['peak_neighborhood_size'], param_grid['fan_value']))\n",
    "for amp_min, peak_neighborhood_size, fan_value in param_combinations:\n",
    "    print(f\"\\nTesting: amp_min={amp_min}, peak_neighborhood_size={peak_neighborhood_size}, fan_value={fan_value}\")\n",
    "    params = {'amp_min': amp_min, 'peak_neighborhood_size': peak_neighborhood_size, 'fan_value': fan_value}\n",
    "    result = evaluate_params(song_samples, song_sr, clip_samples, clip_sr, song.id, params)\n",
    "    results.append(result)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Compute score\n",
    "max_clip_fingerprints = df['clip_fingerprints'].max() if df['clip_fingerprints'].max() > 0 else 1\n",
    "max_song_fingerprints = df['song_fingerprints'].max() if df['song_fingerprints'].max() > 0 else 1\n",
    "df['score'] = df.apply(\n",
    "    lambda row: (\n",
    "        (row['clip_fingerprints'] / max_clip_fingerprints) * 0.3 +\n",
    "        (row['song_fingerprints'] / max_song_fingerprints) * 0.3 +\n",
    "        row['input_confidence'] * 0.2 +\n",
    "        row['db_confidence'] * 0.2\n",
    "    ) if row['match'] else 0, axis=1\n",
    ")\n",
    "\n",
    "# Find best configuration\n",
    "best_config = df.loc[df['score'].idxmax()]\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(best_config)\n",
    "\n",
    "# Display results table\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(df[['amp_min', 'peak_neighborhood_size', 'fan_value', 'clip_fingerprints', 'song_fingerprints', \n",
    "          'match', 'input_confidence', 'db_confidence', 'score', 'reason']])\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 6))\n",
    "for fan_value in param_grid['fan_value']:\n",
    "    subset = df[df['fan_value'] == fan_value]\n",
    "    plt.scatter(subset['amp_min'], subset['input_confidence'], label=f'fan_value={fan_value}', alpha=0.6)\n",
    "plt.xlabel('amp_min')\n",
    "plt.ylabel('Input Confidence (%)')\n",
    "plt.title('Input Confidence vs. amp_min by fan_value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for peak_size in param_grid['peak_neighborhood_size']:\n",
    "    subset = df[df['peak_neighborhood_size'] == peak_size]\n",
    "    plt.scatter(subset['amp_min'], subset['clip_fingerprints'], label=f'peak_size={peak_size}', alpha=0.6)\n",
    "plt.xlabel('amp_min')\n",
    "plt.ylabel('Clip Fingerprints')\n",
    "plt.title('Clip Fingerprints vs. amp_min by peak_neighborhood_size')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Best Configuration\n",
    "\n",
    "Use the best configuration to generate fingerprints and match the clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best parameters\n",
    "best_amp_min = best_config['amp_min']\n",
    "best_peak_neighborhood_size = best_config['peak_neighborhood_size']\n",
    "best_fan_value = best_config['fan_value']\n",
    "\n",
    "print(f\"\\nTesting Best Configuration: amp_min={best_amp_min}, peak_neighborhood_size={best_peak_neighborhood_size}, fan_value={best_fan_value}\")\n",
    "\n",
    "# Generate song fingerprints with visualization\n",
    "song_fingerprints = generate_song_fingerprints(song_samples, song_sr, song.id, amp_min=best_amp_min, \n",
    "                                              peak_neighborhood_size=best_peak_neighborhood_size, \n",
    "                                              fan_value=best_fan_value, plot=True)\n",
    "print(f\"Stored {len(song_fingerprints)} song fingerprints\")\n",
    "\n",
    "# Match clip\n",
    "result = match_clip(clip_samples, clip_sr, song_fingerprints, amp_min=best_amp_min, \n",
    "                    fan_value=best_fan_value, peak_neighborhood_size=best_peak_neighborhood_size)\n",
    "print(\"\\nFinal Matching Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Integration with RadioPlay\n",
    "\n",
    "To use the best configuration in your Django system:\n",
    "1. Update settings.py with the best parameters:\n",
    "   ```python\n",
    "   FINGERPRINT_CONFIG = {\n",
    "       'AMP_MIN': <best_amp_min>,\n",
    "       'PEAK_NEIGHBORHOOD_SIZE': <best_peak_neighborhood_size>,\n",
    "       'FAN_VALUE': <best_fan_value>,\n",
    "       # Other parameters\n",
    "   }\n",
    "   ```\n",
    "2. Save fingerprints to your Fingerprint model:\n",
    "   ```python\n",
    "   for song_id, h, o in db_fingerprints:\n",
    "       Fingerprint.objects.create(song_id=song_id, hash=h, offset=o)\n",
    "   ```\n",
    "3. Adapt match_clip to query your database:\n",
    "   ```python\n",
    "   db_fps = Fingerprint.objects.filter(hash__in=query_hashes)\n",
    "   ```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Test with more songs and noisy clips to validate the configuration.\n",
    "- Adjust the parameter grid or scoring function for specific use cases.\n",
    "- Integrate with your Flutter app for real-time clip processing.\n",
    "- If issues persist, check waveform and spectrogram plots, and share logs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
